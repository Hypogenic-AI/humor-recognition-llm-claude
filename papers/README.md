# Downloaded Papers

This directory contains research papers relevant to investigating low-rank humor recognition in LLMs.

## Paper List

### 1. Linear Representations of Sentiment in Large Language Models
- **File**: `2310.15154_linear_sentiment_representations.pdf`
- **Authors**: Tigges, Hollinsworth, Geiger, Nanda
- **Year**: 2023
- **arXiv**: https://arxiv.org/abs/2310.15154
- **Why relevant**: Core methodology paper demonstrating that sentiment is linearly represented in LLMs. Provides the theoretical foundation and experimental methodology for our humor investigation.

### 2. The Geometry of Truth: Emergent Linear Structure in LLM Representations
- **File**: `2310.06824_geometry_of_truth.pdf`
- **Authors**: Marks, Tegmark
- **Year**: 2024
- **arXiv**: https://arxiv.org/abs/2310.06824
- **Why relevant**: Shows truth/falsehood is linearly represented. Provides probing methodology and transfer experiments we can adapt for humor.

### 3. Sparse Autoencoders Find Highly Interpretable Features in Language Models
- **File**: `2309.08600_sparse_autoencoders_interpretability.pdf`
- **Authors**: Cunningham, Ewart, Riggs, Huben, Sharkey
- **Year**: 2023
- **arXiv**: https://arxiv.org/abs/2309.08600
- **Why relevant**: Methodology for discovering interpretable features in LLM activations. Could be used to identify humor-specific features.

### 4. LoRA: Low-Rank Adaptation of Large Language Models
- **File**: `2106.09685_LoRA_low_rank_adaptation.pdf`
- **Authors**: Hu et al.
- **Year**: 2021
- **arXiv**: https://arxiv.org/abs/2106.09685
- **Why relevant**: Theoretical foundation showing task adaptations have low intrinsic rank. Supports hypothesis that humor recognition is low-rank.

### 5. ColBERT: Using BERT Sentence Embedding for Humor Detection
- **File**: `2004.12765_ColBERT_humor_detection.pdf`
- **Authors**: Annamoradnejad, Zoghi
- **Year**: 2020
- **arXiv**: https://arxiv.org/abs/2004.12765
- **Why relevant**: Baseline humor detection method. Introduces the ColBERT dataset we use for experiments.

### 6. A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models
- **File**: `2407.02646_mechanistic_interpretability_review.pdf`
- **Authors**: Rai et al.
- **Year**: 2024
- **arXiv**: https://arxiv.org/abs/2407.02646
- **Why relevant**: Comprehensive survey of interpretability techniques. Guides methodology selection.

### 7. Humor Detection: A Transformer Gets the Last Laugh
- **File**: `1909.00252_humor_transformer.pdf`
- **Authors**: Weller, Seppi
- **Year**: 2019
- **arXiv**: https://arxiv.org/abs/1909.00252
- **Why relevant**: Early transformer-based humor detection. Historical context.

### 8. Getting Serious about Humor: Crafting Humor Datasets with Unfunny LLMs
- **File**: `2403.00794_getting_serious_humor_datasets.pdf`
- **Authors**: Various
- **Year**: 2024
- **arXiv**: https://arxiv.org/abs/2403.00794
- **Why relevant**: Shows LLMs can distinguish humor internally. Methodology for synthetic data generation.

## Citation Format

When citing these papers in the experiment, use the arXiv IDs or file names for reference.
